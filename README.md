# ğŸŒ¸ Federated Learning with Flower & Hydra

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org)
[![Flower](https://img.shields.io/badge/Flower-FL-green.svg)](https://flower.dev)
[![Hydra](https://img.shields.io/badge/Hydra-Config-orange.svg)](https://hydra.cc)
[![PyTorch](https://img.shields.io/badge/PyTorch-Deep%20Learning-red.svg)](https://pytorch.org)

> ğŸš€ A production-ready federated learning framework combining **Flower** for distributed ML and **Hydra** for configuration management. Train neural networks across multiple clients while keeping data decentralized!

## ğŸ¯ Features

- ğŸ—ï¸ **Modular Architecture**: Clean separation of concerns with dedicated modules
- âš™ï¸ **Flexible Configuration**: Hydra-powered YAML configurations for easy experimentation  
- ğŸ”„ **Federated Learning**: Distributed training using Flower framework
- ğŸ§  **Neural Networks**: CNN implementation for MNIST classification
- ğŸ“Š **Multiple Clients**: Simulate federated scenarios with configurable client numbers
- ğŸ’¾ **Result Persistence**: Automatic saving of training history and metrics

## ğŸ›ï¸ Architecture

ğŸ“‹ ---- Step 1: Configuration

âš™ï¸ base.yaml + toy.yaml â†’ ğŸ›ï¸ Hydra loads configs

ğŸ® ---- Step 2: Orchestration

ğŸ“„ main.py â†’ Coordinates everything
ğŸ“Š dataset.py â†’ Splits MNIST data into client chunks
ğŸ‘¥ client.py â†’ Creates federated learning clients
ğŸ–¥ï¸ server.py â†’ Sets up aggregation strategy

ğŸŒ¸ ---- Step 3: Federated Training

ğŸ¯ Flower server distributes global model to clients
ğŸ‘¤ Each client trains on their local data partition
ğŸ§  model.py â†’ CNN trains on MNIST digits (0-9)
ğŸ“¤ Clients send updated parameters back to server
ğŸ”„ Server aggregates all client updates
ğŸ“ˆ Process repeats for multiple rounds

ğŸ’¾ ---- Step 4: Results

ğŸ“¦ Training history saved to results.pkl
âš™ï¸ Configuration snapshot saved automatically
## ğŸ“ Project Structure

```
ğŸ“¦ federated-learning-project/
â”œâ”€â”€ ğŸ“„ main.py              # Main orchestration script
â”œâ”€â”€ ğŸ® toy.py               # Hydra configuration examples
â”œâ”€â”€ ğŸ“Š dataset.py           # MNIST data loading & partitioning
â”œâ”€â”€ ğŸ‘¥ client.py            # Flower client implementation
â”œâ”€â”€ ğŸ–¥ï¸ server.py            # Server-side functions
â”œâ”€â”€ ğŸ§  model.py             # Neural network definition & training
â”œâ”€â”€ ğŸ“ conf/                # Configuration directory
â”‚   â”œâ”€â”€ âš™ï¸ base.yaml        # Main configuration
â”‚   â””â”€â”€ ğŸ¯ toy.yaml         # Example configurations
â””â”€â”€ ğŸ“ outputs/             # Generated results (auto-created)
```

## ğŸš€ Quick Start

### Prerequisites

```bash
pip install torch torchvision flwr hydra-core omegaconf
```

### ğŸ® Interactive Demo

<details>
<summary>ğŸ¯ Try the Toy Example First!</summary>

```bash
python toy.py
```

This demonstrates Hydra's configuration magic:
- âœ¨ YAML config parsing
- ğŸ”§ Function instantiation  
- ğŸ—ï¸ Object creation from config
- ğŸª Partial function application

</details>

### ğŸƒâ€â™‚ï¸ Run Federated Learning

```bash
# Use default configuration
python main.py

# Override specific parameters
python main.py num_clients=5 num_rounds=10

# Use different config file
python main.py --config-name=custom_config
```

## âš™ï¸ Configuration

### ğŸ“‹ Key Parameters in `base.yaml`

```yaml
# Client Configuration
num_clients: 10                    # Total federated clients
num_clients_per_round_fit: 3       # Clients per training round
num_clients_per_round_eval: 3      # Clients per evaluation round

# Training Parameters  
num_rounds: 3                      # Federated learning rounds
batch_size: 32                     # Local batch size
num_classes: 10                    # MNIST classes (0-9)

# Strategy Configuration
strategy:
  _target_: flwr.server.strategy.FedAvg
  fraction_fit: 0.3
  min_fit_clients: 2
  min_available_clients: 10
```

### ğŸ›ï¸ Customization Examples

<details>
<summary>ğŸ’¡ Click to see configuration examples</summary>

**Change the model:**
```yaml
model:
  _target_: model.Net
  num_classes: 10
```

**Adjust training settings:**
```yaml
config_fit:
  lr: 0.01
  momentum: 0.9  
  local_epochs: 5
```

**Modify federated strategy:**
```yaml
strategy:
  _target_: flwr.server.strategy.FedProx
  proximal_mu: 0.1
```

</details>

## ğŸ§  Model Architecture

The CNN model (`Net` class) features:

- ğŸ”¹ **Conv Layer 1**: 1â†’6 channels, 5Ã—5 kernel
- ğŸ”¹ **Conv Layer 2**: 6â†’16 channels, 5Ã—5 kernel  
- ğŸ”¹ **Max Pooling**: 2Ã—2 after each conv layer
- ğŸ”¹ **Fully Connected**: 16Ã—4Ã—4 â†’ 120 â†’ 84 â†’ 10 classes
- ğŸ”¹ **Activation**: ReLU throughout

## ğŸ“Š Results & Metrics

After training, find your results in:
- ğŸ“ˆ `outputs/{timestamp}/results.pkl` - Complete training history
- ğŸ“‹ `outputs/{timestamp}/.hydra/config.yaml` - Used configuration
- ğŸ“ `outputs/{timestamp}/.hydra/hydra.yaml` - Hydra metadata

## ğŸ”§ Advanced Usage

### ğŸ¯ Custom Strategies

Implement your own federated strategy:

```python
from flwr.server.strategy import Strategy

class MyCustomStrategy(Strategy):
    # Your implementation here
    pass
```

### ğŸ“Š Custom Models

Add new models to `model.py`:

```python
class MyCustomNet(nn.Module):
    def __init__(self, num_classes):
        # Your architecture here
        pass
```

### ğŸ® Experiment Tracking

The framework automatically saves:
- âœ… Training/validation losses
- âœ… Accuracy metrics per client
- âœ… Model parameters after each round
- âœ… Configuration snapshots

## ğŸ› Troubleshooting

<details>
<summary>â— Common Issues & Solutions</summary>

**CUDA Memory Error:**
```bash
# Reduce batch size or use CPU
python main.py batch_size=16
```

**Configuration Not Found:**
```bash
# Check config path
python main.py --config-path=./conf --config-name=base
```

**Client Connection Issues:**
```bash
# Reduce concurrent clients
python main.py client_resources.num_cpus=1
```

</details>

## ğŸ¤ Contributing

1. ğŸ´ Fork the repository
2. ğŸŒŸ Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. ğŸ’¾ Commit your changes (`git commit -m 'Add AmazingFeature'`)
4. ğŸ“¤ Push to the branch (`git push origin feature/AmazingFeature`)
5. ğŸ‰ Open a Pull Request

## ğŸ“š Learn More

- ğŸŒ¸ [Flower Documentation](https://flower.dev/docs/)
- âš¡ [Hydra Documentation](https://hydra.cc/docs/intro/)
- ğŸ”¥ [PyTorch Tutorials](https://pytorch.org/tutorials/)
- ğŸ“– [Federated Learning Paper](https://arxiv.org/abs/1602.05629)

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸŒŸ Acknowledgments

- ğŸŒ¸ **Flower Team** for the fantastic federated learning framework
- âš¡ **Facebook Research** for Hydra configuration management  
- ğŸ”¥ **PyTorch Team** for the deep learning framework
- ğŸ¯ **MNIST Dataset** creators for the benchmark dataset

---

<div align="center">

**â­ Star this repo if you found it helpful! â­**

*Built with â¤ï¸ for the federated learning community*

</div>
